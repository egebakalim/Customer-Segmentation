import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

# Needed so that unpickling the LGBMRegressor works
from lightgbm import LGBMRegressor  # noqa: F401

# ----------------------------------------------------
# Streamlit page config
# ----------------------------------------------------
st.set_page_config(
    page_title="Customer Segmentation & LTV Dashboard",
    layout="wide"
)

sns.set(style="whitegrid")


# ----------------------------------------------------
# Load data & model (cached)
# ----------------------------------------------------
@st.cache_data
def load_rfm():
    rfm = pd.read_csv("rfm_clusters.csv")
    return rfm


@st.cache_resource
def load_model():
    with open("ltv_model.pkl", "rb") as f:
        model = pickle.load(f)
    return model


rfm = load_rfm()
model = load_model()

FEATURES = ["Recency", "Frequency", "Monetary"]


# ----------------------------------------------------
# Sidebar navigation
# ----------------------------------------------------
st.sidebar.title("Navigation")
section = st.sidebar.radio(
    "Go to:",
    ["Overview", "RFM Metrics", "Customer Segments", "LTV Prediction"]
)


# ----------------------------------------------------
# 1. Overview
# ----------------------------------------------------
if section == "Overview":
    st.title("ðŸ›’ Customer Segmentation & Lifetime Value Dashboard")

    st.markdown(
        """
        This dashboard is built on top of the **Online Retail II** dataset and demonstrates:

        - RFM analysis (**Recency, Frequency, Monetary**)
        - **K-Means** customer segmentation
        - **Lifetime Value (LTV)** prediction using a gradient boosting model
        - Exploratory visualizations

        Use the navigation menu on the left to explore each section.
        """
    )

    st.markdown("---")

    st.subheader("Project Summary")
    st.markdown(
        """
        - **Recency**: How many days since the last purchase  
        - **Frequency**: How many invoices the customer has  
        - **Monetary**: Total revenue generated by the customer  

        These metrics are used both for **clustering** and as features for the **LTV prediction model**.
        """
    )


# ----------------------------------------------------
# 2. RFM Metrics
# ----------------------------------------------------
if section == "RFM Metrics":
    st.title("ðŸ“Š RFM Metrics")

    st.write("Below is a sample of the RFM table used for segmentation:")
    st.dataframe(rfm.head())

    col1, col2, col3 = st.columns(3)

    with col1:
        st.write("### Recency Distribution")
        fig, ax = plt.subplots()
        sns.histplot(rfm["Recency"], bins=30, ax=ax)
        ax.set_xlabel("Recency (days)")
        st.pyplot(fig)

    with col2:
        st.write("### Frequency Distribution")
        fig, ax = plt.subplots()
        sns.histplot(rfm["Frequency"], bins=30, ax=ax)
        ax.set_xlabel("Frequency (number of invoices)")
        st.pyplot(fig)

    with col3:
        st.write("### Monetary Distribution")
        fig, ax = plt.subplots()
        sns.histplot(rfm["Monetary"], bins=30, ax=ax)
        ax.set_xlabel("Monetary (total spend)")
        st.pyplot(fig)


# ----------------------------------------------------
# 3. Customer Segments
# ----------------------------------------------------
if section == "Customer Segments":
    st.title("ðŸ§© Customer Segments")

    if "Cluster" not in rfm.columns:
        st.error("The RFM table does not contain a 'Cluster' column.")
    else:
        st.write("### Cluster Counts")
        cluster_counts = rfm["Cluster"].value_counts().sort_index()
        st.bar_chart(cluster_counts)

        st.write("### Average RFM by Cluster")
        cluster_stats = rfm.groupby("Cluster")[FEATURES].mean().round(2)
        st.dataframe(cluster_stats)

        st.write("### Recency vs Monetary by Cluster")
        fig, ax = plt.subplots()
        sns.scatterplot(
            data=rfm,
            x="Recency",
            y="Monetary",
            hue="Cluster",
            palette="tab10",
            ax=ax,
            alpha=0.7
        )
        ax.set_xlabel("Recency (days)")
        ax.set_ylabel("Monetary (total spend)")
        st.pyplot(fig)


# ----------------------------------------------------
# 4. LTV Prediction
# ----------------------------------------------------
if section == "LTV Prediction":
    st.title("ðŸ’° Lifetime Value Prediction")

    st.markdown(
        """
        Use the controls below to simulate a customer and estimate their **future Lifetime Value (LTV)**,
        based on the trained model.
        """
    )

    col1, col2, col3 = st.columns(3)

    with col1:
        recency_input = st.number_input(
            "Recency (days since last purchase)",
            min_value=0,
            max_value=365,
            value=30,
            step=1
        )

    with col2:
        frequency_input = st.number_input(
            "Frequency (number of invoices)",
            min_value=1,
            max_value=500,
            value=5,
            step=1
        )

    with col3:
        monetary_input = st.number_input(
            "Monetary (total historical spend)",
            min_value=0.0,
            max_value=50000.0,
            value=500.0,
            step=10.0
        )

    if st.button("Predict LTV"):
        input_df = pd.DataFrame([{
            "Recency": recency_input,
            "Frequency": frequency_input,
            "Monetary": monetary_input
        }])

        prediction = model.predict(input_df)[0]
        st.success(f"Predicted Lifetime Value: **Â£{prediction:,.2f}**")

    st.markdown("---")

    # Feature importance (if available)
    if hasattr(model, "feature_importances_"):
        st.write("### Feature Importance")
        importance_df = pd.DataFrame({
            "Feature": FEATURES,
            "Importance": model.feature_importances_
        }).sort_values("Importance", ascending=False)

        st.bar_chart(importance_df.set_index("Feature"))
    else:
        st.info("This model does not expose feature importances.")
